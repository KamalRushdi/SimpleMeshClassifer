{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11187518,"sourceType":"datasetVersion","datasetId":6983866}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3D Point Cloud Classification with Simple PointNet","metadata":{}},{"cell_type":"code","source":"!pip install plyfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:05.126199Z","iopub.execute_input":"2025-03-27T16:22:05.126399Z","iopub.status.idle":"2025-03-27T16:22:09.961158Z","shell.execute_reply.started":"2025-03-27T16:22:05.126380Z","shell.execute_reply":"2025-03-27T16:22:09.960117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install open3d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:09.962900Z","iopub.execute_input":"2025-03-27T16:22:09.963233Z","iopub.status.idle":"2025-03-27T16:22:36.304920Z","shell.execute_reply.started":"2025-03-27T16:22:09.963201Z","shell.execute_reply":"2025-03-27T16:22:36.304119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Normalization","metadata":{}},{"cell_type":"code","source":"def normalize_point_cloud(points):\n    \"\"\"\n    Normalize a point cloud to be centered at the origin and fit within a unit sphere.\n\n    :param points: (N, 3) numpy array representing the point cloud.\n    :return: Normalized point cloud.\n    \"\"\"\n    # 1. Centering: Shift the centroid to the origin\n    centroid = np.mean(points, axis=0)\n    points -= centroid\n\n    # 2. Scaling: Normalize by the max distance to the origin\n    max_distance = np.max(np.linalg.norm(points, axis=1))\n    points /= max_distance\n\n    return points, centroid, max_distance","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:20.358695Z","start_time":"2025-03-22T09:09:20.356Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:36.306240Z","iopub.execute_input":"2025-03-27T16:22:36.306524Z","iopub.status.idle":"2025-03-27T16:22:36.311424Z","shell.execute_reply.started":"2025-03-27T16:22:36.306490Z","shell.execute_reply":"2025-03-27T16:22:36.310570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Denormalization","metadata":{}},{"cell_type":"code","source":"def denormalize_point_cloud(points, centroid, max_distance):\n    \"\"\"\n    Denormalize a point cloud to its original coordinates.\n\n    :param points: Normalized point cloud.\n    :param centroid: Centroid of the original point cloud.\n    :param max_distance: Maximum distance from the origin.\n    :return: Denormalized point cloud.\n    \"\"\"\n    # 1. Reverse scaling\n    points *= max_distance\n\n    # 2. Reverse centering\n    points += centroid\n\n    return points","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:20.595896Z","start_time":"2025-03-22T09:09:20.592030Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:36.314363Z","iopub.execute_input":"2025-03-27T16:22:36.314624Z","iopub.status.idle":"2025-03-27T16:22:36.327913Z","shell.execute_reply.started":"2025-03-27T16:22:36.314592Z","shell.execute_reply":"2025-03-27T16:22:36.327218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loader Function","metadata":{}},{"cell_type":"code","source":"# loader function\nimport open3d as o3d\nimport numpy as np\nfrom plyfile import PlyData\nfrom sklearn.neighbors import NearestNeighbors\n\ndef load_and_prepare_data(ply_path, num_points=10000):\n    # Load the PLY file\n    ply_data = PlyData.read(ply_path)\n\n    # Extract vertices and labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n\n    # Load the mesh as an Open3D object\n    mesh = o3d.io.read_triangle_mesh(ply_path)\n\n    # Sample points from the mesh\n    point_cloud = mesh.sample_points_uniformly(number_of_points=num_points)\n\n    # Extract point cloud coordinates\n    points = np.asarray(point_cloud.points)\n\n    # Find nearest neighbors between point cloud and mesh vertices\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(vertices)\n    distances, indices = nbrs.kneighbors(points)\n\n    # Assign labels to the point cloud\n    point_labels = vertex_labels[indices.flatten()]\n\n    return points, point_labels\n","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:41.246022Z","start_time":"2025-03-22T09:09:40.087292Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:36.329491Z","iopub.execute_input":"2025-03-27T16:22:36.329706Z","iopub.status.idle":"2025-03-27T16:22:38.503651Z","shell.execute_reply.started":"2025-03-27T16:22:36.329684Z","shell.execute_reply":"2025-03-27T16:22:38.502911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Network Architecture","metadata":{}},{"cell_type":"code","source":"# network architecture\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass SimplePointNet(nn.Module):\n    def __init__(self, num_classes):\n        super(SimplePointNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Shared MLP for feature extraction\n        self.mlp1 = nn.Sequential(\n            nn.Conv1d(3, 64, 1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Conv1d(128, 1024, 1),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )\n\n        # Fully connected layers for per-point classification\n        self.mlp2 = nn.Sequential(\n            nn.Conv1d(1024, 512, 1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Conv1d(512, 256, 1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Conv1d(256, num_classes, 1)  # Output per point\n        )\n\n    def forward(self, x):\n        # Input shape: (batch_size, 3, num_points)\n        x = self.mlp1(x)  # Shape: (batch_size, 1024, num_points)\n        x = self.mlp2(x)  # Shape: (batch_size, num_classes, num_points)\n\n        return x  # Shape: (batch_size, num_classes, num_points)\n","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:26.860719Z","start_time":"2025-03-22T09:09:24.998901Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:22:42.166300Z","iopub.execute_input":"2025-03-27T16:22:42.166604Z","iopub.status.idle":"2025-03-27T16:22:45.714449Z","shell.execute_reply.started":"2025-03-27T16:22:42.166577Z","shell.execute_reply":"2025-03-27T16:22:45.713492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load data","metadata":{}},{"cell_type":"code","source":"pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:23:06.363472Z","iopub.execute_input":"2025-03-27T16:23:06.363775Z","iopub.status.idle":"2025-03-27T16:23:06.369339Z","shell.execute_reply.started":"2025-03-27T16:23:06.363751Z","shell.execute_reply":"2025-03-27T16:23:06.368620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load point cloud data\nfrom torch.utils.data import Dataset, DataLoader\n\nclass PointCloudDataset(Dataset):\n    def __init__(self, file_paths, num_points=10000):\n        self.file_paths = file_paths\n        self.num_points = num_points\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        ply_path = self.file_paths[idx]\n        points, labels = load_and_prepare_data(ply_path, self.num_points)\n        points, centroid, max_distance = normalize_point_cloud(points)\n        return torch.tensor(points, dtype=torch.float32), torch.tensor(labels, dtype=torch.long), centroid, max_distance\n\n# Example usage\nimport glob\ntrain_path = glob.glob(\"../input/3dmeshs/data/train/*.ply\")\ntest_path = glob.glob(\"../input/3dmeshs/data/val/*.ply\")\ntrain_dataset = PointCloudDataset(train_path)\ntest_dataset = PointCloudDataset(test_path)\ntrain_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=15, shuffle=True)\nprint(len(train_dataset))\nprint(train_dataset.__getitem__(42))","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:45.361799Z","start_time":"2025-03-22T09:09:45.122274Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:23:29.459079Z","iopub.execute_input":"2025-03-27T16:23:29.459369Z","iopub.status.idle":"2025-03-27T16:23:30.028424Z","shell.execute_reply.started":"2025-03-27T16:23:29.459347Z","shell.execute_reply":"2025-03-27T16:23:30.027719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(train_dataset))\ntrain_dataset.__getitem__(0)","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:48.552257Z","start_time":"2025-03-22T09:09:48.334405Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:23:32.999321Z","iopub.execute_input":"2025-03-27T16:23:32.999596Z","iopub.status.idle":"2025-03-27T16:23:33.479666Z","shell.execute_reply.started":"2025-03-27T16:23:32.999575Z","shell.execute_reply":"2025-03-27T16:23:33.478752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Initialize model, loss, and optimizer, and train the model\nmodel = SimplePointNet(num_classes=8)  # Adjust num_classes as needed\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 75\nfor epoch in range(num_epochs):\n    model.train()\n    correct_train = 0\n    total_train = 0\n    all_train_preds = []\n    all_train_labels = []\n\n    for batch_idx, (points, labels, centroid, max_distance) in enumerate(train_loader):\n        points = points.transpose(1, 2)  # (B, 3, N)\n        labels = labels.long()\n\n        outputs = model(points)  # (B, num_classes, N)\n        outputs = outputs.permute(0, 2, 1)  # (B, N, num_classes)\n\n        loss = criterion(outputs.reshape(-1, model.num_classes), labels.view(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accuracy and F1 accumulation\n        preds = outputs.argmax(dim=2)  # (B, N)\n        correct_train += (preds == labels).sum().item()\n        total_train += labels.numel()\n\n        all_train_preds.extend(preds.cpu().numpy().flatten())\n        all_train_labels.extend(labels.cpu().numpy().flatten())\n\n        if batch_idx % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    train_acc = 100 * correct_train / total_train\n    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n\n    # Test evaluation\n    model.eval()\n    correct_test = 0\n    total_test = 0\n    all_test_preds = []\n    all_test_labels = []\n\n    with torch.no_grad():\n        for points, labels, centroid, max_distance in test_loader:\n            points = points.transpose(1, 2)\n            labels = labels.long()\n\n            outputs = model(points)\n            outputs = outputs.permute(0, 2, 1)\n            preds = outputs.argmax(dim=2)\n\n            correct_test += (preds == labels).sum().item()\n            total_test += labels.numel()\n\n            all_test_preds.extend(preds.cpu().numpy().flatten())\n            all_test_labels.extend(labels.cpu().numpy().flatten())\n\n    test_acc = 100 * correct_test / total_test\n    test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] â€” Train Acc: {train_acc:.2f}%, F1: {train_f1:.4f} | Test Acc: {test_acc:.2f}%, F1: {test_f1:.4f}\")\n","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:28.171284Z","start_time":"2025-03-15T14:49:52.163361Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T17:45:22.296092Z","iopub.execute_input":"2025-03-27T17:45:22.296433Z","iopub.status.idle":"2025-03-27T19:09:54.978306Z","shell.execute_reply.started":"2025-03-27T17:45:22.296404Z","shell.execute_reply":"2025-03-27T19:09:54.977478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\n# torch.save(model.state_dict(), \"model_normalize.pth\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:41.845503Z","start_time":"2025-03-15T15:19:41.832971Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T19:11:58.368630Z","iopub.execute_input":"2025-03-27T19:11:58.369055Z","iopub.status.idle":"2025-03-27T19:11:58.381497Z","shell.execute_reply.started":"2025-03-27T19:11:58.369022Z","shell.execute_reply":"2025-03-27T19:11:58.380582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nmodel = SimplePointNet(num_classes=8)\nmodel.load_state_dict(torch.load(\"model_normalize.pth\"))","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:43.093628Z","start_time":"2025-03-15T15:19:43.063200Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T19:12:01.393854Z","iopub.execute_input":"2025-03-27T19:12:01.394192Z","iopub.status.idle":"2025-03-27T19:12:01.412279Z","shell.execute_reply.started":"2025-03-27T19:12:01.394164Z","shell.execute_reply":"2025-03-27T19:12:01.411465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation and testing","metadata":{}},{"cell_type":"markdown","source":"### Point Cloud Metrics","metadata":{}},{"cell_type":"code","source":"# point cloud accuracy\nimport os\nfrom sklearn.metrics import f1_score\nimport sys\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Directory containing validation files\nval_dir = \"../input/3dmeshs/data/val\"\nval_files = glob.glob(os.path.join(val_dir, \"*.ply\"))\n\n# Initialize lists to store metrics\naccuracies = []\nf1_scores = []\nsamples_points = {}\nsamples_predicted_labels = {}\n# for mesh metrics\ncentroids = {}\nmax_distances = {}\n\n# flush the output\nsys.stdout.flush()\n# Iterate over all validation files\nfor file_path in val_files:\n    # Load data\n    points, true_labels = load_and_prepare_data(file_path, num_points=10000)\n    samples_points.update({os.path.basename(file_path): points})\n    points, centroid, max_distance = normalize_point_cloud(points)\n    centroids.update({os.path.basename(file_path): centroid})\n    max_distances.update({os.path.basename(file_path): max_distance})\n    # Prepare input tensor\n\n    points = torch.tensor(points, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n    points = points.transpose(1, 2)  # Shape: (1, 3, num_points)\n\n    # Predict labels\n    with torch.no_grad():\n        outputs = model(points)\n        predicted_labels = torch.argmax(outputs, dim=1)  # Shape: (1, num_points)\n        samples_predicted_labels.update({os.path.basename(file_path): predicted_labels})\n\n    # Convert ground truth labels to tensor\n    true_labels = torch.tensor(true_labels, dtype=torch.long)  # Shape: (num_points,)\n\n    # Calculate accuracy\n    correct = (predicted_labels == true_labels).sum().item()\n    total = true_labels.size(0)\n    accuracy = correct / total\n    accuracies.append(accuracy)\n\n    # Calculate F1 score\n    f1 = f1_score(true_labels.numpy(), predicted_labels.squeeze(0).numpy(), average=\"weighted\")\n    f1_scores.append(f1)\n\n    print(f\"File: {os.path.basename(file_path)} | Accuracy: {accuracy * 100:.2f}% | F1 Score: {f1:.4f}\")\n\n# Calculate average accuracy and F1 score\navg_accuracy = np.mean(accuracies)\navg_f1_score = np.mean(f1_scores)\n\nprint(f\"\\nAverage Accuracy: {avg_accuracy * 100:.2f}%\")\nprint(f\"Average F1 Score: {avg_f1_score:.4f}\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:22:58.139368Z","start_time":"2025-03-15T15:22:54.896297Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T19:12:04.710359Z","iopub.execute_input":"2025-03-27T19:12:04.710658Z","iopub.status.idle":"2025-03-27T19:12:11.697922Z","shell.execute_reply.started":"2025-03-27T19:12:04.710634Z","shell.execute_reply":"2025-03-27T19:12:11.697107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Mesh Metrics","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom plyfile import PlyData\nimport glob\n\n# Directories\nval_dir = \"data/val\"\nresults_dir = \"results/predicted labels\"\nos.makedirs(results_dir, exist_ok=True)  # Create results directory if it doesn't exist\n\n# Get all PLY files in the validation directory\nval_files = glob.glob(os.path.join(val_dir, \"*.ply\"))\n\n# Initialize lists to store metrics\nall_accuracies = []\nall_f1_scores = []\n\nall_true_labels = []\nall_predicted_labels = []\n\n# Iterate over all validation files\nfor file_path in val_files:\n    # Load the PLY file\n    ply_data = PlyData.read(file_path)\n\n    # Extract vertices and true labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n    all_true_labels.append(vertex_labels) # for classification report\n\n    points = samples_points[os.path.basename(file_path)]\n    points = denormalize_point_cloud(points, centroids[os.path.basename(file_path)], max_distances[os.path.basename(file_path)])\n    # Find nearest neighbors between mesh vertices and point cloud\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(points)\n    distances, indices = nbrs.kneighbors(vertices)\n\n    # Assign labels to the mesh vertices\n    predicted_vertex_labels = samples_predicted_labels[os.path.basename(file_path)].flatten()[indices.flatten()]\n    all_predicted_labels.append(predicted_vertex_labels) # for classification report\n\n    # Save predicted labels to a file\n    sample_name = os.path.splitext(os.path.basename(file_path))[0]\n    output_file = os.path.join(results_dir, f\"{sample_name}_labels.txt\")\n    np.savetxt(output_file, predicted_vertex_labels, fmt='%d')\n\n    # Calculate accuracy and F1 score for this sample\n    accuracy = accuracy_score(vertex_labels, predicted_vertex_labels)\n    f1 = f1_score(vertex_labels, predicted_vertex_labels, average=\"weighted\")\n\n    # Append to lists\n    all_accuracies.append(accuracy)\n    all_f1_scores.append(f1)\n    print(f\"Sample: {sample_name} | Accuracy: {accuracy * 100:.2f}% | F1 Score: {f1:.4f}\")\n\n# Calculate average metrics\navg_accuracy = np.mean(all_accuracies)\navg_f1_score = np.mean(all_f1_scores)\n\n# Print overall metrics\nprint(\"\\nOverall Metrics:\")\nprint(f\"Average Accuracy: {avg_accuracy * 100:.2f}%\")\nprint(f\"Average F1 Score: {avg_f1_score:.4f}\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:24:27.873883Z","start_time":"2025-03-15T15:24:25.149973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclass_names = [\"head\", \"neck\", \"torso\", \"left_arm\", \"right_arm\", \"hip\", \"legs\"]\nprint(classification_report(np.concatenate(all_true_labels), np.concatenate(all_predicted_labels), target_names=class_names))","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:24:34.301872Z","start_time":"2025-03-15T15:24:34.232327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute confusion matrix\ncm = confusion_matrix(np.concatenate(all_true_labels), np.concatenate(all_predicted_labels))\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:47:50.063721Z","start_time":"2025-03-15T15:47:48.567149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Predictions and Ground Truth","metadata":{}},{"cell_type":"code","source":"# generates colored output for either original mesh or predicted mesh\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\ndef generate_colored_output(mesh_path, predicted_labels_path = None):\n    sample_name = mesh_path.split(\"/\")[-1].split(\".\")[0]\n\n    # Load the original mesh PLY file\n    ply_data = PlyData.read(mesh_path)\n\n    # Extract vertices and original labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    if predicted_labels_path is not None:\n        # predicted_nameofthesameple_colored.ply\n        vertex_labels = np.loadtxt(predicted_labels_path, dtype=int)\n        output_name = f\"predicted_{sample_name}_colored.ply\"\n        output_dir = \"results/predicted colored\"\n        output_path = os.path.join(output_dir, output_name)\n    else:\n        # nameofthesample_colored.ply\n        vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n        output_name = f\"{sample_name}_colored.ply\"\n        output_dir = \"results/original colored\"\n        output_path = os.path.join(output_dir, output_name)\n\n\n    # Define class colors (RGB format, range 0-255)\n    class_colors = {\n        1: (255, 0, 0),    # Red\n        2: (0, 255, 0),    # Green\n        3: (0, 0, 255),    # Blue\n        4: (255, 255, 0),  # Yellow\n        5: (255, 165, 0),  # Orange\n        6: (128, 0, 128),  # Purple\n        7: (0, 255, 255),  # Cyan\n    }\n\n    # Assign colors based on predicted labels\n    colors = np.array([class_colors[label] for label in vertex_labels], dtype=np.uint8)\n\n    # Create a new structured array for the vertex data\n    vertex_data = np.empty(len(vertices), dtype=[\n        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),  # Vertex positions\n        ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),  # Vertex colors\n        ('label', 'i4')  # Predicted labels\n    ])\n\n    # Fill vertex data with positions, colors, and predicted labels\n    vertex_data['x'] = vertices[:, 0]\n    vertex_data['y'] = vertices[:, 1]\n    vertex_data['z'] = vertices[:, 2]\n    vertex_data['red'] = colors[:, 0]\n    vertex_data['green'] = colors[:, 1]\n    vertex_data['blue'] = colors[:, 2]\n    vertex_data['label'] = vertex_labels\n\n    # Extract faces from the original mesh (if it's a mesh)\n    if 'face' in ply_data:\n        faces = ply_data['face']['vertex_indices']\n        face_data = np.array([(tuple(face),) for face in faces], dtype=[('vertex_indices', 'i4', (3,))])\n    else:\n        face_data = None  # No faces (point cloud)\n\n    # Save the updated mesh with new colors and labels\n    new_ply_vertices = PlyElement.describe(vertex_data, 'vertex')\n    if face_data is not None:\n        new_ply_faces = PlyElement.describe(face_data, 'face')\n        PlyData([new_ply_vertices, new_ply_faces]).write(output_path)\n    else:\n        PlyData([new_ply_vertices]).write(output_path)\n\n    print(f\"Updated mesh saved as '{output_name}' in the '{output_dir}' directory\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:31:40.107309Z","start_time":"2025-03-15T15:31:40.089577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate colored output for either original mesh or predicted mesh\nimport os\n\nmesh_dir = \"data/val\"\npredicted_labels_dir = \"results/predicted labels\"\nfor file in os.listdir(mesh_dir):\n    if file.endswith(\".ply\"):\n        file_path = os.path.join(mesh_dir, file)\n        predicted_labels_file = os.path.join(predicted_labels_dir, f\"{os.path.splitext(file)[0]}_labels.txt\")\n        generate_colored_output(file_path, predicted_labels_file)\n        generate_colored_output(file_path)\n","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:31:52.935858Z","start_time":"2025-03-15T15:31:43.002611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}