{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11187518,"sourceType":"datasetVersion","datasetId":6983866}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3D Point Cloud Classification with Simple PointNet","metadata":{}},{"cell_type":"code","source":"!pip install plyfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:38:45.432922Z","iopub.execute_input":"2025-03-29T16:38:45.433253Z","iopub.status.idle":"2025-03-29T16:38:48.951497Z","shell.execute_reply.started":"2025-03-29T16:38:45.433224Z","shell.execute_reply":"2025-03-29T16:38:48.950405Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: plyfile in /usr/local/lib/python3.10/dist-packages (1.1)\nRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->plyfile) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21->plyfile) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21->plyfile) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21->plyfile) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21->plyfile) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21->plyfile) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install open3d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:38:51.612158Z","iopub.execute_input":"2025-03-29T16:38:51.612539Z","iopub.status.idle":"2025-03-29T16:39:14.036455Z","shell.execute_reply.started":"2025-03-29T16:38:51.612510Z","shell.execute_reply":"2025-03-29T16:39:14.035347Z"}},"outputs":[{"name":"stdout","text":"Collecting open3d\n  Using cached open3d-0.19.0-cp310-cp310-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\nCollecting dash>=2.6.0 (from open3d)\n  Using cached dash-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.1.3)\nRequirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.1.0)\nRequirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\nCollecting configargparse (from open3d)\n  Using cached ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.5)\nCollecting addict (from open3d)\n  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.0.0)\nRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.5)\nRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.3)\nRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\nRequirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.67.1)\nCollecting pyquaternion (from open3d)\n  Using cached pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\nCollecting flask>=3.0.0 (from open3d)\n  Using cached flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\nCollecting werkzeug>=3.0.0 (from open3d)\n  Using cached werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\nCollecting retrying (from dash>=2.6.0->open3d)\n  Using cached retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\nRequirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.0->open3d) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.0->open3d) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->open3d) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\nDownloading open3d-0.19.0-cp310-cp310-manylinux_2_31_x86_64.whl (447.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dash-3.0.1-py3-none-any.whl (8.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\nDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\nDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\nInstalling collected packages: addict, werkzeug, retrying, configargparse, flask, dash, pyquaternion, open3d\n  Attempting uninstall: werkzeug\n    Found existing installation: Werkzeug 3.1.3\n    Uninstalling Werkzeug-3.1.3:\n      Successfully uninstalled Werkzeug-3.1.3\n  Attempting uninstall: flask\n    Found existing installation: Flask 3.1.0\n    Uninstalling Flask-3.1.0:\n      Successfully uninstalled Flask-3.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed addict-2.4.0 configargparse-1.7 dash-3.0.1 flask-3.0.3 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Normalization","metadata":{}},{"cell_type":"code","source":"def normalize_point_cloud(points):\n    \"\"\"\n    Normalize a point cloud to be centered at the origin and fit within a unit sphere.\n\n    :param points: (N, 3) numpy array representing the point cloud.\n    :return: Normalized point cloud.\n    \"\"\"\n    # 1. Centering: Shift the centroid to the origin\n    centroid = np.mean(points, axis=0)\n    points -= centroid\n\n    # 2. Scaling: Normalize by the max distance to the origin\n    max_distance = np.max(np.linalg.norm(points, axis=1))\n    points /= max_distance\n\n    return points, centroid, max_distance","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:20.358695Z","start_time":"2025-03-22T09:09:20.356Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:34:52.640215Z","iopub.execute_input":"2025-03-29T19:34:52.640619Z","iopub.status.idle":"2025-03-29T19:34:52.645412Z","shell.execute_reply.started":"2025-03-29T19:34:52.640589Z","shell.execute_reply":"2025-03-29T19:34:52.644393Z"}},"outputs":[],"execution_count":148},{"cell_type":"markdown","source":"Denormalization","metadata":{}},{"cell_type":"code","source":"def denormalize_point_cloud(points, centroid, max_distance):\n    \"\"\"\n    Denormalize a point cloud to its original coordinates.\n\n    :param points: Normalized point cloud.\n    :param centroid: Centroid of the original point cloud.\n    :param max_distance: Maximum distance from the origin.\n    :return: Denormalized point cloud.\n    \"\"\"\n    # 1. Reverse scaling\n    points *= max_distance\n\n    # 2. Reverse centering\n    points += centroid\n\n    return points","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:20.595896Z","start_time":"2025-03-22T09:09:20.592030Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:34:54.727092Z","iopub.execute_input":"2025-03-29T19:34:54.727442Z","iopub.status.idle":"2025-03-29T19:34:54.731673Z","shell.execute_reply.started":"2025-03-29T19:34:54.727413Z","shell.execute_reply":"2025-03-29T19:34:54.730680Z"}},"outputs":[],"execution_count":149},{"cell_type":"markdown","source":"Rotation augmentation","metadata":{}},{"cell_type":"code","source":"def rotate_pointcloud_3d(pc, max_degrees=60):\n    \"\"\" Apply a random 3D rotation \"\"\"\n    max_angle = np.deg2rad(max_degrees)  # Converts 30 degrees to radians\n    angles = np.random.uniform(-max_angle, max_angle, size=3)\n    Rx = np.array([\n        [1, 0, 0],\n        [0, np.cos(angles[0]), -np.sin(angles[0])],\n        [0, np.sin(angles[0]),  np.cos(angles[0])]\n    ])\n    Ry = np.array([\n        [np.cos(angles[1]), 0, np.sin(angles[1])],\n        [0, 1, 0],\n        [-np.sin(angles[1]), 0, np.cos(angles[1])]\n    ])\n    Rz = np.array([\n        [np.cos(angles[2]), -np.sin(angles[2]), 0],\n        [np.sin(angles[2]),  np.cos(angles[2]), 0],\n        [0, 0, 1]\n    ])\n    R = Rz @ Ry @ Rx\n    return pc @ R.T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:34:55.820641Z","iopub.execute_input":"2025-03-29T19:34:55.820952Z","iopub.status.idle":"2025-03-29T19:34:55.827165Z","shell.execute_reply.started":"2025-03-29T19:34:55.820929Z","shell.execute_reply":"2025-03-29T19:34:55.826312Z"}},"outputs":[],"execution_count":150},{"cell_type":"markdown","source":"Jitter augmentation","metadata":{}},{"cell_type":"code","source":"def jitter_pointcloud(pc, sigma=0.01, clip=0.02):\n    jitter = np.clip(sigma * np.random.randn(*pc.shape), -clip, clip)\n    return pc + jitter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:34:58.133100Z","iopub.execute_input":"2025-03-29T19:34:58.133462Z","iopub.status.idle":"2025-03-29T19:34:58.137699Z","shell.execute_reply.started":"2025-03-29T19:34:58.133434Z","shell.execute_reply":"2025-03-29T19:34:58.136681Z"}},"outputs":[],"execution_count":151},{"cell_type":"markdown","source":"Loader Function","metadata":{}},{"cell_type":"code","source":"# loader function\nimport open3d as o3d\nimport numpy as np\nfrom plyfile import PlyData\nfrom sklearn.neighbors import NearestNeighbors\n\ndef load_and_prepare_data(ply_path, num_points=10000):\n    # Load the PLY file\n    ply_data = PlyData.read(ply_path)\n\n    # Extract vertices and labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n    \n    # Load the mesh as an Open3D object\n    mesh = o3d.io.read_triangle_mesh(ply_path)\n\n    # Sample points from the mesh\n    point_cloud = mesh.sample_points_uniformly(number_of_points=num_points)\n\n    # Extract point cloud coordinates\n    points = np.asarray(point_cloud.points)\n\n    # Find nearest neighbors between point cloud and mesh vertices\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(vertices)\n    distances, indices = nbrs.kneighbors(points)\n\n    # Assign labels to the point cloud\n    point_labels = vertex_labels[indices.flatten()]\n\n    return points, point_labels\n","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:41.246022Z","start_time":"2025-03-22T09:09:40.087292Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:56:51.929257Z","iopub.execute_input":"2025-03-29T19:56:51.929616Z","iopub.status.idle":"2025-03-29T19:56:51.935550Z","shell.execute_reply.started":"2025-03-29T19:56:51.929589Z","shell.execute_reply":"2025-03-29T19:56:51.934473Z"}},"outputs":[],"execution_count":172},{"cell_type":"markdown","source":"Network Architecture","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TNet(nn.Module):\n    def __init__(self, k=3):\n        super(TNet, self).__init__()\n        self.k = k\n        self.conv1 = nn.Conv1d(k, 64, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 1024, 1)\n        self.bn3 = nn.BatchNorm1d(1024)\n\n        self.fc1 = nn.Linear(1024, 512)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn5 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, k * k)\n\n        self.fc3.weight.data.zero_()\n        self.fc3.bias.data.copy_(torch.eye(k).view(-1))\n\n    def forward(self, x):\n        B = x.size(0)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2)[0]\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n        return x.view(B, self.k, self.k)\n\nclass SimplePointNet(nn.Module):\n    def __init__(self, num_classes):\n        super(SimplePointNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Input T-Net\n        self.input_transform = TNet(k=3)\n\n        # Shared MLP for feature extraction\n        self.mlp1 = nn.Sequential(\n            nn.Conv1d(3, 64, 1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Conv1d(128, 1024, 1),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )\n\n        # Fully connected layers for per-point classification\n        self.mlp2 = nn.Sequential(\n            nn.Conv1d(1024, 512, 1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Conv1d(512, 256, 1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Conv1d(256, num_classes, 1)  # Output per point\n        )\n\n    def forward(self, x):\n        # Input shape: (batch_size, 3, num_points)\n        B, _, N = x.shape\n\n        # Apply input transform\n        trans = self.input_transform(x)  # (B, 3, 3)\n        x = torch.bmm(trans, x)          # Apply to point cloud\n\n        x = self.mlp1(x)                 # (B, 1024, N)\n        x = self.mlp2(x)                 # (B, num_classes, N)\n        return x\n","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:26.860719Z","start_time":"2025-03-22T09:09:24.998901Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:56:53.453169Z","iopub.execute_input":"2025-03-29T19:56:53.453515Z","iopub.status.idle":"2025-03-29T19:56:53.464375Z","shell.execute_reply.started":"2025-03-29T19:56:53.453482Z","shell.execute_reply":"2025-03-29T19:56:53.463326Z"}},"outputs":[],"execution_count":173},{"cell_type":"markdown","source":"Load data","metadata":{}},{"cell_type":"code","source":"print(torch.cuda.is_available())  # Should be True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:56:56.861670Z","iopub.execute_input":"2025-03-29T19:56:56.861973Z","iopub.status.idle":"2025-03-29T19:56:56.866768Z","shell.execute_reply.started":"2025-03-29T19:56:56.861950Z","shell.execute_reply":"2025-03-29T19:56:56.865848Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":174},{"cell_type":"code","source":"# load point cloud data\nfrom torch.utils.data import Dataset, DataLoader\n\nclass PointCloudDataset(Dataset):\n    def __init__(self, file_paths, num_points=10000, training=False):\n        self.file_paths = file_paths\n        self.num_points = num_points\n        self.training = training  # Flag to control augmentation\n        \n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        ply_path = self.file_paths[idx]\n        points, labels = load_and_prepare_data(ply_path, self.num_points)\n        points, centroid, max_distance = normalize_point_cloud(points)\n        # augmetnation\n        if self.training:\n            points = rotate_pointcloud_3d(points)\n            # points = jitter_pointcloud(points)\n        return (\n            torch.tensor(points, dtype=torch.float32), \n            torch.tensor(labels, dtype=torch.long), \n            centroid, \n            max_distance\n        )\n\n# Example usage\nimport glob\ntrain_path = glob.glob(\"../input/3dmeshs/data/train/*.ply\")\ntest_path = glob.glob(\"../input/3dmeshs/data/val/*.ply\")\ntrain_dataset = PointCloudDataset(train_path, num_points=20000, training = True)\ntest_dataset = PointCloudDataset(test_path,num_points=20000, training = False)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True , pin_memory=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True , pin_memory=True, num_workers=4)\nprint(len(train_dataset))\nprint(train_dataset.__getitem__(42))","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:45.361799Z","start_time":"2025-03-22T09:09:45.122274Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:56:58.629038Z","iopub.execute_input":"2025-03-29T19:56:58.629399Z","iopub.status.idle":"2025-03-29T19:56:59.121544Z","shell.execute_reply.started":"2025-03-29T19:56:58.629368Z","shell.execute_reply":"2025-03-29T19:56:59.120675Z"}},"outputs":[{"name":"stdout","text":"47\n(tensor([[ 3.0106e-04, -2.3772e-01,  3.6141e-01],\n        [ 4.3673e-01,  5.3433e-01, -2.7173e-01],\n        [-1.2831e-01, -2.9907e-01,  3.8387e-01],\n        ...,\n        [ 2.8113e-01,  4.9219e-01, -1.8879e-01],\n        [ 1.7332e-01, -1.9961e-02, -2.2385e-01],\n        [ 2.8377e-01,  2.8347e-01, -6.1637e-01]]), tensor([3, 7, 3,  ..., 6, 3, 7]), array([ 0.49930935,  0.1758292 , -0.42444072]), 0.5033892068195674)\n","output_type":"stream"}],"execution_count":175},{"cell_type":"code","source":"print(type(train_dataset))\nprint(len(train_dataset.__getitem__(0)[0]))","metadata":{"ExecuteTime":{"end_time":"2025-03-22T09:09:48.552257Z","start_time":"2025-03-22T09:09:48.334405Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:57:01.449409Z","iopub.execute_input":"2025-03-29T19:57:01.449719Z","iopub.status.idle":"2025-03-29T19:57:01.926641Z","shell.execute_reply.started":"2025-03-29T19:57:01.449695Z","shell.execute_reply":"2025-03-29T19:57:01.925824Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.PointCloudDataset'>\n20000\n","output_type":"stream"}],"execution_count":176},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"print(\"Unique labels in batch:\", labels.unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:57:04.089256Z","iopub.execute_input":"2025-03-29T19:57:04.089606Z","iopub.status.idle":"2025-03-29T19:57:04.107287Z","shell.execute_reply.started":"2025-03-29T19:57:04.089577Z","shell.execute_reply":"2025-03-29T19:57:04.106019Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-177-5a0e96d3a944>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique labels in batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"],"ename":"NameError","evalue":"name 'labels' is not defined","output_type":"error"}],"execution_count":177},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Initialize model, loss, and optimizer\nmodel = SimplePointNet(num_classes=8).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\nnum_epochs = 25\nfor epoch in range(num_epochs):\n    model.train()\n    correct_train = 0\n    total_train = 0\n    all_train_preds = []\n    all_train_labels = []\n\n    for batch_idx, (points, labels, centroid, max_distance) in enumerate(train_loader):\n        points = points.transpose(1, 2).to(device)      # (B, 3, N)\n        labels = labels.long().to(device)\n\n        outputs = model(points)                         # (B, num_classes, N)\n        outputs = outputs.permute(0, 2, 1)              # (B, N, num_classes)\n\n        loss = criterion(outputs.reshape(-1, model.num_classes), labels.view(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accuracy and F1 accumulation\n        preds = outputs.argmax(dim=2)                   # (B, N)\n        correct_train += (preds == labels).sum().item()\n        total_train += labels.numel()\n\n        all_train_preds.extend(preds.cpu().numpy().flatten())\n        all_train_labels.extend(labels.cpu().numpy().flatten())\n\n        if batch_idx % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    train_acc = 100 * correct_train / total_train\n    train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n\n    # Test evaluation\n    model.eval()\n    correct_test = 0\n    total_test = 0\n    total_test_loss = 0.0\n    all_test_preds = []\n    all_test_labels = []\n\n    with torch.no_grad():\n        for points, labels, centroid, max_distance in test_loader:\n            points = points.transpose(1, 2).to(device)\n            labels = labels.long().to(device)\n\n            outputs = model(points)\n            outputs = outputs.permute(0, 2, 1)\n\n            loss = criterion(outputs.reshape(-1, model.num_classes), labels.view(-1))\n            total_test_loss += loss.item() * labels.size(0)  # multiply by batch size\n            \n            preds = outputs.argmax(dim=2)\n            correct_test += (preds == labels).sum().item()\n            total_test += labels.numel()\n\n            all_test_preds.extend(preds.cpu().numpy().flatten())\n            all_test_labels.extend(labels.cpu().numpy().flatten())\n\n    test_loss = total_test_loss / len(test_loader.dataset)\n    test_acc = 100 * correct_test / total_test\n    test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] — \"\n          f\"Train Acc: {train_acc:.2f}%, F1: {train_f1:.4f} | \"\n          f\"Test Acc: {test_acc:.2f}%, F1: {test_f1:.4f}, Loss: {test_loss:.4f}\")\n","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:28.171284Z","start_time":"2025-03-15T14:49:52.163361Z"},"execution":{"iopub.status.busy":"2025-03-29T20:06:09.021462Z","iopub.execute_input":"2025-03-29T20:06:09.021780Z","iopub.status.idle":"2025-03-29T20:14:29.484531Z","shell.execute_reply.started":"2025-03-29T20:06:09.021756Z","shell.execute_reply":"2025-03-29T20:14:29.483449Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch [1/25], Batch [0/3], Loss: 2.1553\nEpoch [1/25] — Train Acc: 44.26%, F1: 0.4783 | Test Acc: 39.59%, F1: 0.2245, Loss: 2.0211\nEpoch [2/25], Batch [0/3], Loss: 0.8888\nEpoch [2/25] — Train Acc: 71.43%, F1: 0.7048 | Test Acc: 39.53%, F1: 0.2240, Loss: 1.8680\nEpoch [3/25], Batch [0/3], Loss: 0.9099\nEpoch [3/25] — Train Acc: 70.34%, F1: 0.6923 | Test Acc: 39.62%, F1: 0.2248, Loss: 1.7583\nEpoch [4/25], Batch [0/3], Loss: 0.8346\nEpoch [4/25] — Train Acc: 73.13%, F1: 0.7197 | Test Acc: 39.61%, F1: 0.2376, Loss: 1.7270\nEpoch [5/25], Batch [0/3], Loss: 0.6953\nEpoch [5/25] — Train Acc: 74.89%, F1: 0.7388 | Test Acc: 46.86%, F1: 0.3273, Loss: 1.4208\nEpoch [6/25], Batch [0/3], Loss: 0.6136\nEpoch [6/25] — Train Acc: 77.13%, F1: 0.7625 | Test Acc: 55.72%, F1: 0.4499, Loss: 1.1529\nEpoch [7/25], Batch [0/3], Loss: 0.5503\nEpoch [7/25] — Train Acc: 79.38%, F1: 0.7866 | Test Acc: 71.02%, F1: 0.6557, Loss: 0.8324\nEpoch [8/25], Batch [0/3], Loss: 0.5390\nEpoch [8/25] — Train Acc: 79.61%, F1: 0.7888 | Test Acc: 73.20%, F1: 0.6746, Loss: 0.7963\nEpoch [9/25], Batch [0/3], Loss: 0.5788\nEpoch [9/25] — Train Acc: 81.45%, F1: 0.8065 | Test Acc: 72.16%, F1: 0.6764, Loss: 0.8098\nEpoch [10/25], Batch [0/3], Loss: 0.6517\nEpoch [10/25] — Train Acc: 74.18%, F1: 0.7343 | Test Acc: 67.85%, F1: 0.6651, Loss: 0.8187\nEpoch [11/25], Batch [0/3], Loss: 0.4694\nEpoch [11/25] — Train Acc: 79.83%, F1: 0.7899 | Test Acc: 72.80%, F1: 0.7240, Loss: 0.6961\nEpoch [12/25], Batch [0/3], Loss: 0.5567\nEpoch [12/25] — Train Acc: 80.15%, F1: 0.7935 | Test Acc: 71.00%, F1: 0.7066, Loss: 0.7665\nEpoch [13/25], Batch [0/3], Loss: 0.4753\nEpoch [13/25] — Train Acc: 81.62%, F1: 0.8072 | Test Acc: 71.82%, F1: 0.7125, Loss: 0.7460\nEpoch [14/25], Batch [0/3], Loss: 0.5215\nEpoch [14/25] — Train Acc: 79.45%, F1: 0.7857 | Test Acc: 73.51%, F1: 0.7303, Loss: 0.6918\nEpoch [15/25], Batch [0/3], Loss: 0.5494\nEpoch [15/25] — Train Acc: 79.70%, F1: 0.7895 | Test Acc: 80.35%, F1: 0.7970, Loss: 0.5270\nEpoch [16/25], Batch [0/3], Loss: 0.5279\nEpoch [16/25] — Train Acc: 81.47%, F1: 0.8073 | Test Acc: 82.68%, F1: 0.8189, Loss: 0.4790\nEpoch [17/25], Batch [0/3], Loss: 0.5709\nEpoch [17/25] — Train Acc: 77.52%, F1: 0.7672 | Test Acc: 78.23%, F1: 0.7703, Loss: 0.5685\nEpoch [18/25], Batch [0/3], Loss: 0.4968\nEpoch [18/25] — Train Acc: 78.44%, F1: 0.7751 | Test Acc: 74.86%, F1: 0.7298, Loss: 0.6641\nEpoch [19/25], Batch [0/3], Loss: 0.6106\nEpoch [19/25] — Train Acc: 78.94%, F1: 0.7807 | Test Acc: 71.78%, F1: 0.6948, Loss: 0.7228\nEpoch [20/25], Batch [0/3], Loss: 0.5105\nEpoch [20/25] — Train Acc: 78.83%, F1: 0.7802 | Test Acc: 72.37%, F1: 0.6953, Loss: 0.7033\nEpoch [21/25], Batch [0/3], Loss: 0.4961\nEpoch [21/25] — Train Acc: 77.63%, F1: 0.7678 | Test Acc: 74.68%, F1: 0.7161, Loss: 0.6731\nEpoch [22/25], Batch [0/3], Loss: 0.4660\nEpoch [22/25] — Train Acc: 80.22%, F1: 0.7938 | Test Acc: 76.85%, F1: 0.7462, Loss: 0.6175\nEpoch [23/25], Batch [0/3], Loss: 0.3743\nEpoch [23/25] — Train Acc: 81.92%, F1: 0.8112 | Test Acc: 82.51%, F1: 0.8139, Loss: 0.4894\nEpoch [24/25], Batch [0/3], Loss: 0.4695\nEpoch [24/25] — Train Acc: 83.28%, F1: 0.8251 | Test Acc: 81.17%, F1: 0.8010, Loss: 0.5148\nEpoch [25/25], Batch [0/3], Loss: 0.4397\nEpoch [25/25] — Train Acc: 81.09%, F1: 0.8031 | Test Acc: 78.38%, F1: 0.7715, Loss: 0.6132\n","output_type":"stream"}],"execution_count":180},{"cell_type":"markdown","source":"Flush the gpu memory in notebook training","metadata":{}},{"cell_type":"code","source":"# print(torch.cuda.memory_allocated()) \n# del  loss, optimizer, model, points, labels\n# print(torch.cuda.memory_allocated())  # Current memory in use\n\n# import gc\n# # Force garbage collection\n# gc.collect()\n\n# # Clear PyTorch's cache\n# torch.cuda.empty_cache()\n# print(torch.cuda.memory_allocated())  # Current memory in use","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:05:42.973843Z","iopub.execute_input":"2025-03-29T20:05:42.974168Z","iopub.status.idle":"2025-03-29T20:05:43.319636Z","shell.execute_reply.started":"2025-03-29T20:05:42.974145Z","shell.execute_reply":"2025-03-29T20:05:43.318853Z"}},"outputs":[{"name":"stdout","text":"47225856\n19439616\n19439616\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n    reader_close()\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n","output_type":"stream"}],"execution_count":179},{"cell_type":"code","source":"# Save model\n# torch.save(model.state_dict(), \"model_normalize.pth\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:41.845503Z","start_time":"2025-03-15T15:19:41.832971Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:15:34.639653Z","iopub.execute_input":"2025-03-29T20:15:34.640032Z","iopub.status.idle":"2025-03-29T20:15:34.685985Z","shell.execute_reply.started":"2025-03-29T20:15:34.640001Z","shell.execute_reply":"2025-03-29T20:15:34.685302Z"}},"outputs":[],"execution_count":181},{"cell_type":"code","source":"# Load model\nmodel = SimplePointNet(num_classes=8)\nmodel.load_state_dict(torch.load(\"model_normalize.pth\"))","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:19:43.093628Z","start_time":"2025-03-15T15:19:43.063200Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:15:35.125369Z","iopub.execute_input":"2025-03-29T20:15:35.125709Z","iopub.status.idle":"2025-03-29T20:15:35.165764Z","shell.execute_reply.started":"2025-03-29T20:15:35.125683Z","shell.execute_reply":"2025-03-29T20:15:35.164922Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-182-8f24460a96a2>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"model_normalize.pth\"))\n","output_type":"stream"},{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":182},{"cell_type":"markdown","source":"## Evaluation and testing","metadata":{}},{"cell_type":"markdown","source":"### Point Cloud Metrics","metadata":{}},{"cell_type":"code","source":"# point cloud accuracy\nimport os\nfrom sklearn.metrics import f1_score\nimport sys\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Directory containing validation files\nval_dir = \"../input/3dmeshs/data/val\"\nval_files = glob.glob(os.path.join(val_dir, \"*.ply\"))\n\n# Initialize lists to store metrics\naccuracies = []\nf1_scores = []\nsamples_points = {}\nsamples_predicted_labels = {}\n# for mesh metrics\ncentroids = {}\nmax_distances = {}\n\n# flush the output\nsys.stdout.flush()\n# Iterate over all validation files\nfor file_path in val_files:\n    # Load data\n    points, true_labels = load_and_prepare_data(file_path, num_points=10000)\n    samples_points.update({os.path.basename(file_path): points})\n    points, centroid, max_distance = normalize_point_cloud(points)\n    centroids.update({os.path.basename(file_path): centroid})\n    max_distances.update({os.path.basename(file_path): max_distance})\n    # Prepare input tensor\n\n    points = torch.tensor(points, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n    points = points.transpose(1, 2)  # Shape: (1, 3, num_points)\n\n    # Predict labels\n    with torch.no_grad():\n        outputs = model(points)\n        predicted_labels = torch.argmax(outputs, dim=1)  # Shape: (1, num_points)\n        samples_predicted_labels.update({os.path.basename(file_path): predicted_labels})\n\n    # Convert ground truth labels to tensor\n    true_labels = torch.tensor(true_labels, dtype=torch.long)  # Shape: (num_points,)\n\n    # Calculate accuracy\n    correct = (predicted_labels == true_labels).sum().item()\n    total = true_labels.size(0)\n    accuracy = correct / total\n    accuracies.append(accuracy)\n\n    # Calculate F1 score\n    f1 = f1_score(true_labels.numpy(), predicted_labels.squeeze(0).numpy(), average=\"weighted\")\n    f1_scores.append(f1)\n\n    print(f\"File: {os.path.basename(file_path)} | Accuracy: {accuracy * 100:.2f}% | F1 Score: {f1:.4f}\")\n\n# Calculate average accuracy and F1 score\navg_accuracy = np.mean(accuracies)\navg_f1_score = np.mean(f1_scores)\n\nprint(f\"\\nAverage Accuracy: {avg_accuracy * 100:.2f}%\")\nprint(f\"Average F1 Score: {avg_f1_score:.4f}\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:22:58.139368Z","start_time":"2025-03-15T15:22:54.896297Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:15:39.394661Z","iopub.execute_input":"2025-03-29T20:15:39.394973Z","iopub.status.idle":"2025-03-29T20:15:48.270131Z","shell.execute_reply.started":"2025-03-29T20:15:39.394948Z","shell.execute_reply":"2025-03-29T20:15:48.269057Z"}},"outputs":[{"name":"stdout","text":"File: raw_59.ply | Accuracy: 83.76% | F1 Score: 0.8287\nFile: raw_58.ply | Accuracy: 77.95% | F1 Score: 0.7694\nFile: raw_50.ply | Accuracy: 80.98% | F1 Score: 0.7982\nFile: raw_52.ply | Accuracy: 83.86% | F1 Score: 0.8287\nFile: raw_57.ply | Accuracy: 73.46% | F1 Score: 0.7119\nFile: raw_54.ply | Accuracy: 57.83% | F1 Score: 0.5582\nFile: raw_53.ply | Accuracy: 84.35% | F1 Score: 0.8322\nFile: raw_60.ply | Accuracy: 78.59% | F1 Score: 0.7610\nFile: raw_55.ply | Accuracy: 81.31% | F1 Score: 0.7996\nFile: raw_56.ply | Accuracy: 79.84% | F1 Score: 0.7927\nFile: raw_51.ply | Accuracy: 81.83% | F1 Score: 0.8061\n\nAverage Accuracy: 78.52%\nAverage F1 Score: 0.7715\n","output_type":"stream"}],"execution_count":183},{"cell_type":"markdown","source":"### Mesh Metrics","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom plyfile import PlyData\nimport glob\n\n# Directories\nval_dir = \"data/val\"\nresults_dir = \"results/predicted labels\"\nos.makedirs(results_dir, exist_ok=True)  # Create results directory if it doesn't exist\n\n# Get all PLY files in the validation directory\nval_files = glob.glob(os.path.join(val_dir, \"*.ply\"))\n\n# Initialize lists to store metrics\nall_accuracies = []\nall_f1_scores = []\n\nall_true_labels = []\nall_predicted_labels = []\n\n# Iterate over all validation files\nfor file_path in val_files:\n    # Load the PLY file\n    ply_data = PlyData.read(file_path)\n\n    # Extract vertices and true labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n    all_true_labels.append(vertex_labels) # for classification report\n\n    points = samples_points[os.path.basename(file_path)]\n    points = denormalize_point_cloud(points, centroids[os.path.basename(file_path)], max_distances[os.path.basename(file_path)])\n    # Find nearest neighbors between mesh vertices and point cloud\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(points)\n    distances, indices = nbrs.kneighbors(vertices)\n\n    # Assign labels to the mesh vertices\n    predicted_vertex_labels = samples_predicted_labels[os.path.basename(file_path)].flatten()[indices.flatten()]\n    all_predicted_labels.append(predicted_vertex_labels) # for classification report\n\n    # Save predicted labels to a file\n    sample_name = os.path.splitext(os.path.basename(file_path))[0]\n    output_file = os.path.join(results_dir, f\"{sample_name}_labels.txt\")\n    np.savetxt(output_file, predicted_vertex_labels, fmt='%d')\n\n    # Calculate accuracy and F1 score for this sample\n    accuracy = accuracy_score(vertex_labels, predicted_vertex_labels)\n    f1 = f1_score(vertex_labels, predicted_vertex_labels, average=\"weighted\")\n\n    # Append to lists\n    all_accuracies.append(accuracy)\n    all_f1_scores.append(f1)\n    print(f\"Sample: {sample_name} | Accuracy: {accuracy * 100:.2f}% | F1 Score: {f1:.4f}\")\n\n# Calculate average metrics\navg_accuracy = np.mean(all_accuracies)\navg_f1_score = np.mean(all_f1_scores)\n\n# Print overall metrics\nprint(\"\\nOverall Metrics:\")\nprint(f\"Average Accuracy: {avg_accuracy * 100:.2f}%\")\nprint(f\"Average F1 Score: {avg_f1_score:.4f}\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:24:27.873883Z","start_time":"2025-03-15T15:24:25.149973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclass_names = [\"head\", \"neck\", \"torso\", \"left_arm\", \"right_arm\", \"hip\", \"legs\"]\nprint(classification_report(np.concatenate(all_true_labels), np.concatenate(all_predicted_labels), target_names=class_names))","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:24:34.301872Z","start_time":"2025-03-15T15:24:34.232327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute confusion matrix\ncm = confusion_matrix(np.concatenate(all_true_labels), np.concatenate(all_predicted_labels))\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:47:50.063721Z","start_time":"2025-03-15T15:47:48.567149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Predictions and Ground Truth","metadata":{}},{"cell_type":"code","source":"# generates colored output for either original mesh or predicted mesh\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\ndef generate_colored_output(mesh_path, predicted_labels_path = None):\n    sample_name = mesh_path.split(\"/\")[-1].split(\".\")[0]\n\n    # Load the original mesh PLY file\n    ply_data = PlyData.read(mesh_path)\n\n    # Extract vertices and original labels\n    vertices = np.vstack([\n        ply_data['vertex']['x'],\n        ply_data['vertex']['y'],\n        ply_data['vertex']['z']\n    ]).T  # Shape: (N, 3)\n\n    if predicted_labels_path is not None:\n        # predicted_nameofthesameple_colored.ply\n        vertex_labels = np.loadtxt(predicted_labels_path, dtype=int)\n        output_name = f\"predicted_{sample_name}_colored.ply\"\n        output_dir = \"results/predicted colored\"\n        output_path = os.path.join(output_dir, output_name)\n    else:\n        # nameofthesample_colored.ply\n        vertex_labels = np.array(ply_data['vertex']['label'])  # Shape: (N,)\n        output_name = f\"{sample_name}_colored.ply\"\n        output_dir = \"results/original colored\"\n        output_path = os.path.join(output_dir, output_name)\n\n\n    # Define class colors (RGB format, range 0-255)\n    class_colors = {\n        1: (255, 0, 0),    # Red\n        2: (0, 255, 0),    # Green\n        3: (0, 0, 255),    # Blue\n        4: (255, 255, 0),  # Yellow\n        5: (255, 165, 0),  # Orange\n        6: (128, 0, 128),  # Purple\n        7: (0, 255, 255),  # Cyan\n    }\n\n    # Assign colors based on predicted labels\n    colors = np.array([class_colors[label] for label in vertex_labels], dtype=np.uint8)\n\n    # Create a new structured array for the vertex data\n    vertex_data = np.empty(len(vertices), dtype=[\n        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),  # Vertex positions\n        ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),  # Vertex colors\n        ('label', 'i4')  # Predicted labels\n    ])\n\n    # Fill vertex data with positions, colors, and predicted labels\n    vertex_data['x'] = vertices[:, 0]\n    vertex_data['y'] = vertices[:, 1]\n    vertex_data['z'] = vertices[:, 2]\n    vertex_data['red'] = colors[:, 0]\n    vertex_data['green'] = colors[:, 1]\n    vertex_data['blue'] = colors[:, 2]\n    vertex_data['label'] = vertex_labels\n\n    # Extract faces from the original mesh (if it's a mesh)\n    if 'face' in ply_data:\n        faces = ply_data['face']['vertex_indices']\n        face_data = np.array([(tuple(face),) for face in faces], dtype=[('vertex_indices', 'i4', (3,))])\n    else:\n        face_data = None  # No faces (point cloud)\n\n    # Save the updated mesh with new colors and labels\n    new_ply_vertices = PlyElement.describe(vertex_data, 'vertex')\n    if face_data is not None:\n        new_ply_faces = PlyElement.describe(face_data, 'face')\n        PlyData([new_ply_vertices, new_ply_faces]).write(output_path)\n    else:\n        PlyData([new_ply_vertices]).write(output_path)\n\n    print(f\"Updated mesh saved as '{output_name}' in the '{output_dir}' directory\")","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:31:40.107309Z","start_time":"2025-03-15T15:31:40.089577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate colored output for either original mesh or predicted mesh\nimport os\n\nmesh_dir = \"data/val\"\npredicted_labels_dir = \"results/predicted labels\"\nfor file in os.listdir(mesh_dir):\n    if file.endswith(\".ply\"):\n        file_path = os.path.join(mesh_dir, file)\n        predicted_labels_file = os.path.join(predicted_labels_dir, f\"{os.path.splitext(file)[0]}_labels.txt\")\n        generate_colored_output(file_path, predicted_labels_file)\n        generate_colored_output(file_path)\n","metadata":{"ExecuteTime":{"end_time":"2025-03-15T15:31:52.935858Z","start_time":"2025-03-15T15:31:43.002611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}